{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIforMed_W3_Theory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhksXp+gHmfbQ2O1HdsJtM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabeMaldonado/AIforMedicine/blob/master/AIforMed_W3_Theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEREGTbKX4k5",
        "colab_type": "text"
      },
      "source": [
        "# Notes on AI for Medicine -- Week 3\n",
        "\n",
        "## MRI Images\n",
        "\n",
        "MRI images are not 2D images like X-rays, an MRI sequence is a 3D image that consists of multiple 3D volumes. We can think of these volumes/sequences as channels similar to RGB channels in a regular image. However, an MRI image can contain more than 3 channels. These channels get stacked in the depth dimension to create an MRI image. One challenge when combining sequences is that the images can be misaligned due to the patients moving or tilting their heads when the scan is being performed.  If the images are not aligned with each other when we combine them-- the brain region at one location in  one channel would not correspond to the same region of the brain in the other channels. To fix this problem we use a technique called ***image registration*** \n",
        "The idea behind **Image Registration** is to transform the images so that they are aligned or registered with each other. Once the images have been registered we are able to combine the different sequences. This technique can be applied to all the slices of the brain. Now that these data have been combined then we can turn to the task of ***segmentation***\n",
        "\n",
        "## Brain Tumor Segmentation\n",
        "\n",
        "Segmentation is the process of defining the boundaries of various tissues. In this case we are trying to define the boundaries of tumors. We can also think of segmentation as the task of determining the class of every point in the 3D volume. These points-- in a 2D space are called *pixels* and in a 3D space are called ***voxels*** \n",
        "There are two approaches to segmentation with MRI data: \n",
        "1.  2D Segmentation. In this approach we break the 3D MRI volume into many 2D slices. Each one of these slices is passed to the segmentation model which outputs the segmentation for that slice. One by one, each slice is passed to the segmentation model to generate the segmentation for each slice. These 2D slices can then again be combined to form a 3D output volume of the segmentation. \n",
        "   * The drawback with this approach is that we can lose important 3D context. For instance, if there is a tumor in one slice, there is likely to be a tumor in the slices adjacent to it. Since we are passing in slices one at a time to the network, the network is not able to learn this useful context. \n",
        "2.  3D Segmentation. In this approach, we would want to, ideally, pass the entire MRI volume into the segmentation model and get a 3D segmentation map of the whole MRI as output. However, due to the size of the entire MRI volume it makes it nearly impossible to pass it in all at once into the model. It would be too expensive computationally and memory-wise. To solve this issue we can break up the 3D MRI volume into many 3D sub volumes. Each of these sub volumes would have some width, height, and depth context. Like in the 2D approach, we can now feed these sub volumes one at the time to the model and then aggregate them at the end to form a segmentation map of the whole volume. \n",
        "   *   The disadvantage with this 3D approach is that we might still lose some spatial context. For instance, if there is a tumor in one sub volume, there is likely to be a tumor in the surrounding volumes too. Since we are passing sub volumes one at the time into the network, the network might not be able to learn from possibly useful context. \n",
        "   *   The advantage with this approach is that we are capturing some context in all of the width, height, and depth dimensions.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HbPQ1DhxpWl",
        "colab_type": "text"
      },
      "source": [
        "## Segmentation Architectures\n",
        "\n",
        "One of the most popular architectures for segmentation is **U-Net** which was first designed for biomedical image segmentation and has performed well in the task and cell tracking. U-Net can achieve good results even with only hundreds of examples. The U-Net architecture consists of two paths (starting at the top left side of the U):\n",
        "1.   Contracting path. This is a typical Convolutional Neural Network like the ones used in image classification. It consists of repeating applications of convolution and pooling operations. The convolution operation here is called *down convolution*. The key here is that in the contracting path the feature maps get spatially smaller which is why it is called a ***contraction*** \n",
        "2.   Expanding path. This path is doing the opposite of the contracting path as it takes the small feature maps through a series of up-sampling and up-convolution steps to get back to the original size of the image. It also concatenates the up-sample representations at each step with the corresponding feature maps at the contraction pathway. \n",
        "At the last step, the architecture outputs the probability of tumor for every pixel in the image. \n",
        "The U-Net architecture can be trained on input/output pairs of 2D slice if using the 2D approach. We can also use the 3D approach by replacing the 2D operations with their 3D counterparts. This is exactly what the **3D U-Net** architecture does. The 2D convolutions become 3D convolutions and the 2D pooling layers become 3D pooling layers. 3D U-Net allows us to pass in 3D sub volumes and get an output for every voxel in the volume  specifying the probability of a tumor. \n",
        "\n",
        "![alt text](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
        "[For more info visit the U-Net creators' site](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM2jc4zo0OgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}