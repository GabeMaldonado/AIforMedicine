{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIforMed_C2_Theory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUgieKA3CLoiCoULPHr+KL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabeMaldonado/AIforMedicine/blob/master/AIforMed_C2_Theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5xvEsMfcAl4",
        "colab_type": "text"
      },
      "source": [
        "$\\mathfrak{Gabriel \\ Maldonado}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlLe_5MJb3Ys",
        "colab_type": "text"
      },
      "source": [
        "# Notes on AI for Medicine Specialization, Coursera\n",
        "## Course II -- AI for Medical Prognosis\n",
        "\n",
        "Prognosis is a branch of medicine that specializes in predicting the future health of patients. For example, given a patient's labs results-- can we estimate the risks of having a heart attack over the next five or ten years. \n",
        "Machine Learning is a powerful tool for prognosis and can provide a temendous boost to this branch of medicine by using many different types of medical data to make accurate predictions about a patient's future health. \n",
        "Making prognosis is a clinically useful task for a variety of reasons. \n",
        "1.   **Risk of Illness** It is useful for informing patients the risk of develping an illness. There are blood tests that are used to estimate the risk of developing breasts and ovarian cancer.\n",
        "2.   **SUrvival with Illness** It is also used to inform patients how long they can expect to survive with a certain illness. Cancer staging gives an estimate of survival time of a patient with a particular type of cancer. \n",
        "\n",
        "Prognosis is also used for guiding treatment. In clinical practice, the prediction of a 10-year risk of heart attack is used to determine whether that patient should get drugs to reduce that risk. Another example is the **6-month mortality risk**. This is used for patients with terminal conditions that have become advanced and uncurable and it is used to determine who should recieve end-of-life care. \n",
        "\n",
        "### Prognosis -- Inputs and Outputs\n",
        "\n",
        "Prognostic models can be thought of as a system that takes in a profile of a patient as an input and outputs s **risk score** for that patient. The patient profile can include:\n",
        "*   Clinical History which includes major illness, previous medical procedures. \n",
        "*   Physical Examinations -- includes vital signs such as temperature and blodd pressure. \n",
        "*   Labs and imaging -- inlcudes blood work and CT scans, etc. \n",
        "The prognostic model can take one or more of these pieces of information and produce the patient's risk score which can be an arbitrary number or a probability. \n",
        "\n",
        "### Calculating $CHA_{2}DS_{2}-VASc$ (Chads vasc) Score\n",
        "\n",
        "Here we will calculate the risk factor for patients with **atrial fibrilation**. \n",
        "Atrial Fibrilation is a common abnormal heart rhythm that puts the patient at the risk of stroke. (Stroke is when the blood flow to a region of the brain is cut off). The CHADS VASC model is used to calculate the one year risk of stroke for patients with AF. The name comes from the following conditions and the numbers in the columns represent the coefficients for each condition and the patient score for each condition (1 = yes, 0 = no). The last column is the coefficient * value and these products get added to obtain the risk factor.\n",
        "\n",
        "<pre>\n",
        "*   C - Congestive Heart Failure         1     0     0\n",
        "*   H - Hypertension                     1     1     1\n",
        "*   A2 - Age 75 years or older           2     0     0\n",
        "*   D - Diabetis mellitus                1     1     1\n",
        "*   S2 - Stroke, TIA or TE               2     0     0\n",
        "*   V - Vascular disease                 1     0     0\n",
        "*   A - Age 65 to 74                     1     1     1\n",
        "*   Sc - Sex category (female)           1     0     0\n",
        "*                                                   ---\n",
        "*                                                    3\n",
        "</pre>\n",
        "\n",
        "### Calculating the Model for End-stage liver disease (MELD) Score\n",
        "\n",
        "This score gives an  estimate of the 3-month mortality for patients older than 12 who are on liver transplant waiting lists. This score is a factor in determining how quickly a patient can get a liver transplant. \n",
        "\n",
        "Let's calculat the score for a 50 year old woman with the following lab results:\n",
        "*   Creatinine = 1.0 mg/dL\n",
        "*   Bilirubin total = 2.0 mg/dL\n",
        "*   INR = 1.1\n",
        "\n",
        "<pre>\n",
        "                     Coeff    Value        Coeff * Value\n",
        "ln(Creatinine)       0.957    ln(1.0)            0\n",
        "ln(Bilirubin total)  0.378    ln(2.0)           0.26 \n",
        "ln(INR)              1.120    ln(1.1)           0.11\n",
        "Intercept            0.643       1              0.643\n",
        "-----------------------------------------------------------\n",
        "Score * 10                                 1.01x10 = 10   \n",
        "</pre>\n",
        "\n",
        "This score of 10 is not directly telling us the probability of survival at 3 months but it is informative when comparing it to MELD score of other patients. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9qvrZaqvyAi",
        "colab_type": "text"
      },
      "source": [
        "[Check out this notebook to see how to calculate risk scores in python ](https://github.com/GabeMaldonado/AIforMedicine/blob/master/AIforMed_C2_W1_Lab2.ipynb)\n",
        "\n",
        "[Combining Features in pandas](https://github.com/GabeMaldonado/AIforMedicine/blob/master/AIforMed_C2_W1_Lab3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lDpQ_F97U4P",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating Prognostic Models\n",
        "\n",
        "Teh basic idea behind evaluating a prognostinc model is to see how well it performs on pairs of patients. Now, to evaluate these risks scores we need to know whether the patients actually had the event. Here we are looking at death within 10 years so we need to know if patient **A** died within the next 10 years but patient **B** did not. A good prognostic model should give patient A a higher score than to patient B. \n",
        "In general when the patient with the worst outcome has a higher risk score, this pair is called ***Concordant***. Now, when the patient with the worst outcome does not have a higher score the pair is called ***Not Concordant/Discordant***. Patient with same scores are called **risk ties**.\n",
        "A pair where the outcomes are different is called a **permissible** pair. It's with such pairs that we can evaluate prognostic models. \n",
        "\n",
        "#### Evaluating the prognostic model:\n",
        "\n",
        "#### C-Index\n",
        "*   +1 for a permissible pair that is concordant\n",
        "*   +0.5 for a permissible pair for risk tie. \n",
        "\n",
        "$$ C Index = \\frac{\\# concordant\\  pairs + 0.5 \\times \\# risk \\ ties }{\\# permissible\\  pairs\u001e} $$\n",
        "\n",
        "### C-Index Interpretation \n",
        "\n",
        "$$ P (score (A) > score(B)  | Y_{A}>Y_{B}$$\n",
        "*   Random Model score = 0.5\n",
        "*   Perfect model score = 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdvHe_ucFPiw",
        "colab_type": "text"
      },
      "source": [
        "[Calculating C-Index Lab](https://github.com/GabeMaldonado/AIforMedicine/blob/master/AIforMed_C2_W1_Lab4.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ4dg12ZptmO",
        "colab_type": "text"
      },
      "source": [
        "## Decision Trees for Prognosis\n",
        "\n",
        "We can use Decision Trees to build machine learning models. Decision trees are extremely useful in medical applications due to their ability to handle both continuous and categorical data, their interpretability, and the speed at which we can train them. \n",
        "\n",
        "Often ML models are considered blac boxes due to their complex inner workings, but in medicine, the ability to explain interpret a model may be critical for human acceptance and trust. \n",
        "We will build a prognostic model using age and systolic blood pressure that will predict a 10-year mortality risk. \n",
        "\n",
        "*   Systolic blood pressure is the pressure in your blood vessels when your heart beats. \n",
        "\n",
        "In this case, the decision tree would divide the input space into three regions of high-risk and low-risk using vertical and horizontal boundaries. The classifier can also be represented as a tree with an if-then structure. The decision tree is asking a series of questions and classifies the patient based on the answers to such questions. \n",
        "\n",
        "**How's a decision tree built?**\n",
        "At a high level:\n",
        "*   Pick a variable  and a value of that variable that partitions the data, such that one partition contains mostly red and the other partition contains mostly blue. We pick the variable and the value based on how well it splits the data, blues to one side and reds to the other. \n",
        "*   Repeat the same process in another region of the data until the partitions are mostly red and blue. \n",
        "*   Estimate risk in each partition. \n",
        "*   Binarize the output to just output whether an area is low or high risk. We can call a prediction high-risk if the predicted probability is greater than 50% and low-risk otherwise. \n",
        "\n",
        "### Challenges when building Decision Trees\n",
        "\n",
        "One major challenge when bulding decision trees is that if we don't stop growing the decision trees, they continue to create more and more partitions and they can get overly complex. Decision models can create overly complex trees that fit the training data almost prefectly! This ends up being a bad thing adn it is known as ***overfitting***. The model fits the data so well that is unable to generalize other samples or real-world data. \n",
        "One way to combat overfitting is to control when we stop growing the trees. We cna control this by setting the maximum depth the tree can grow to. Another popular way to combat overfitting is y building a ***Random Forest***. \n",
        "\n",
        "### Random Forests \n",
        "\n",
        "Random Forests construct multiple decision trees and average their risk predictions. \n",
        "How can a random forest be trained? There are two key concepts when training a random forests. \n",
        "1.   Each tree in the forest is constructed using a random sample of the patients. For instance, for the 1st tree, we might draw P1, P2 & P1 which can happen because the random forest samples with replacement.\n",
        "2.   The random forest algorithm also modifies the splitting procedure in the construction of the decision tree such that it uses a subset of features when creating decision boundaries. \n",
        "\n",
        "Random Forests generally boost the performance over single trees. With a single decision tree we might get a test accuracy of 0.71 with a random forest of 100 trees we can get an accuracy of 0.76. \n",
        "\n",
        "Random Forests are called an ***ensemble*** learning method because they use multiple decision trees to obtain better prediction performance than could be obtain from any of the decision trees alone. \n",
        "\n",
        "There are other popular algorithms that use ensembles including ** Gradient Boosting, XGBoost, LightGBM** which are also able to achieve high performance when working with structure data in medicine and other domains. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZkL_PHqLEqC",
        "colab_type": "text"
      },
      "source": [
        "[Link for W2 Lab1 which cover Missing Values, Imputation, Masks, and Decision Trees](https://github.com/GabeMaldonado/AIforMedicine/blob/master/AIforMed_C2_W2_Lab1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWCd7hWnLbXO",
        "colab_type": "text"
      },
      "source": [
        "## Survival Data\n",
        "\n",
        "To model survival, we need to represent the data in a form in which we can process. The primary challenge here is censored data which is a particular form of missing data. \n",
        "In healthcare, missing data is a common yet important issue we have to deal with. Let's imagine we are working with incomplete patient data. If we run these data through a 'regular' machine learning pipeline to create a prognostic model-- we would:\n",
        "*   Create a train/test split\n",
        "*   Exclude missing data, drop NaNs / rows\n",
        "*   Create and run/train the model\n",
        "*   Evaluate model using the test set\n",
        "*   Let's say the model achieves a train_accuracy=0.87 and a test-accuracy=0.84. We notice that the accury for both sets is relatively high. \n",
        "\n",
        "Now let's say that we get a new test set which has been collected using the same method as the previous set but this new set contains no missing values. If we run this new test set through our random forest model, it would achieve a low accuracy of say, 0.61. What's causing this considering the model performed pretty well when using the first sets?\n",
        "This is because the distributions of the data, for the old and new test sets, are different. To check this, we can look at the distribution of the input variables. Comparing the distribution graphs for the variables in the older and new set would allow us to see the difference in the data. If we find any discrepancies in the graph, let's say-- the graph for the new data shows more patients under 40, the we should take a second look at the old data and compare the pre and post dropped NaNs to see how the data compares. It might be the case that looking at these two graphs we will see that before dropping missing data, the set contain more values for patients under 40. When we dropped the missing values, we also eliminated data for a lot of young patients. The missing data could be due to the fact that in the field, medical practitioners might not record BP data for youn patients but do so for every older pantient. We need to examine the data carefully before procedding with the ML pipeline so we can avoid buliding a biased model. \n",
        "\n",
        "## Why is Data Missing?\n",
        "\n",
        "To decide whether a complete case analysis would lead to bias we need to understand ***why data are missing***. There are three missing data categories:\n",
        "\n",
        "1.   Missing Completely at Random -- Missing data is not dependent on anything. For instance, a doctor can flip a coin to decide to record the BP for any patient, regardless of age, without any given criteria. Here the probability of missing data is constant, let's say 50% chance that the doctor will forget to record the BP data for a patient. When we have data that is missing completely at random, it would not lead to a biased model. $$p(missing) = constant$$\n",
        "2.   Missing at Random -- The missing data is dependent on a condition. For instance the doctor would decide to always record the BP if the patient is older than 40. For patients younger than 40, he can use a flip coin as above to decide whether or not to record the BP data. Here the probability of missing data is not constant as the missingness in the data is determined by the condition, in this case-- age. $$p(missing) \\neq constant$$ \n",
        "$$p (missing | age < 40) = 0.5 \\neq p(missing|age>40)=0 $$\n",
        "3.   Missing Not at Random -- The missing data is caused by an unobservable event/unavailable information. For instance, in a particularly busy day at the doctor's office, the doctor can decide to record the BP data by the flip of a coin. If the office is not busy then there will be plenty of time to record the BP for every patient. \n",
        "$p(missing) \\neq constant$$ \n",
        "$$p (missing | busy) = 0.5 \\neq p(missing|not \\ busy)=0 $$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBXRg_HurEia",
        "colab_type": "text"
      },
      "source": [
        "## Imputation\n",
        "\n",
        "An alternative to complete case analysis is to complete or **inpute** the missing values. Imputation replaces missign data with an estimated valued based on other available information. The two primary imputation methods are:\n",
        "*   Mean Imputation -- replaces missing values with the mean of the data feature. Note that for the test set, we shoudl replace the missing values with the mean of the data of the train set as the amount of data in the test set is less and it might not be representative of all of the data. Keep in mind that mean imputation is not preserving the relationship between the variables. All the imputed values would lie in a staight line (same mean value for all) across all the ages. \n",
        "\n",
        "*   Regression Imputation -- tries to learn a linear model of the form: \n",
        "$$BP =  coefficient_{age} \\times age + offset$$ and it will replace the missing values with the result of that linear function. Let's say that the linear equation we have is:$$BP= 0.6 \\times age + 115$$\n",
        "We'd replace the missing value for a patient of age $57$ with:\n",
        "$$BP = 0.6 \\times 57 + 115 = 149$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf1R1W6a98bL",
        "colab_type": "text"
      },
      "source": [
        "### Right Censoring\n",
        "The time to an event is only known to exceed a certain value. There are two types of right censoring:\n",
        "*   End-of-study censoring -- where the patient completes the study\n",
        "*   Loss-to-follow-up censoring -- where the patient dropsmout before the end of the study \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foPsZAfEC4Ln",
        "colab_type": "text"
      },
      "source": [
        "## Hazard Functions\n",
        "\n",
        "### Survival Probability:\n",
        "\n",
        "$$S(t) = Pr(T > t)$$\n",
        "\n",
        "### Survival to Hazard\n",
        "Hazard , represented by the Greek letter lamba $\\lambda$, is the risk of death if aged $t$\n",
        "$$\\lambda(t) = Pr(T =t | T \\geq t )$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x9n24BV7Tat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "|"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}