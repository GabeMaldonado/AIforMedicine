{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIforMed_W2_Theory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTOJ0YpfSFb2LHl4nEBapH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabeMaldonado/AIforMedicine/blob/master/AIforMed_W2_Theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTOQdXhlYqzz",
        "colab_type": "text"
      },
      "source": [
        "# Notes on AI for Medicine -- Week 2\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "**How can we determine how good a model is?**\n",
        "To answer that question, we can start by looking at **ACCURACY**\n",
        "\n",
        "$$ Accuracy = \\dfrac {Examples  \\ of \\ correctly \\ classified} {Total \\ Number\\  of \\ Samples}$$\n",
        "\n",
        "We can interpret *accuracy* as the probability of being correct:\n",
        "\n",
        "$$ Accuracy = P(correct)$$\n",
        "\n",
        "We can break down this further as the sum of two probaibilities:\n",
        "\n",
        "$$ Accuracy = P(correct \\cap disease) + P (correct \\cap normal)$$\n",
        "\n",
        "Using the law of conditional probability we can expand this further:\n",
        "\n",
        "$$Using P(A \\cap B) = P( A | B) P(B)$$\n",
        "$$Accuracy = P(correct|disease) P(disease) + P(correct|normal)P(normal)$$\n",
        "$$Accuracy = P(+|disease) P(disease) + P(-|normal)P(normal)$$\n",
        "\n",
        "P(+|disease) => Sensitivity(true positive rate)\n",
        "P(-|normal)  => Specificity(true negative rate)\n",
        "\n",
        "***Sensitivity*** is the probability that the model classifies the patient as having the disease given they have the disease. \n",
        "***Specificity*** is the probability that the model classifies the patient as not having the disease given they don't have the disease.\n",
        "\n",
        "We now arrive at: \n",
        "\n",
        "$$Accuracy = Sensitivity \\  \\ast \\ P(disease) \\ + \\ Specificity\\ \\ast\\ P(normal)  $$\n",
        "\n",
        "Where the probability of a patient having a disease in the population is called ***prevalence*** and the probaility of being normal is 1 - prevalence.\n",
        "\n",
        "$$Accuracy = Sensitivity\\ \\ast \\ prevalence\\ +\\ Specificity\\ \\ast\\ (1 - prevalance)$$\n",
        "\n",
        "The equation above is useful because it allows us to see the probability as a weighted average of sensitivity and specificity. The weight associated with Sensitivity is the prevalance and the weight associated with Specificity is 1 minus the prevalance. This equation also alows us to find any of these quantities given the other three quantities. \n",
        "\n",
        "**Sensitivity** tells us that if a patient has the disease , what is the probability that the model predicts positive?\n",
        "$$P (+ | disease) $$\n",
        "\n",
        "Bur a doctor using the model might have this question -- if a model prediction is positive, what is the probability that a patient has the disease? This is called the **Positive Predictive Value (PPV)** of the model.\n",
        "$$P(disease | +)$$\n",
        "\n",
        "**Specificity** asks if a patient is normal, what is the probability that the model predicts negative?\n",
        "$$P(-|normal)$$\n",
        "In this case the doctor might be interested in knowing that if a model prediction is negative, what is the probability that a patient is normal? This is called the **Negative Predictive value (NPV)** of the model.\n",
        "$$P(normal|-)$$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_qAWUeBmbiy",
        "colab_type": "text"
      },
      "source": [
        "## Calculating the PPV in terms of Sensitivity, Specificity and Prevalence\n",
        "\n",
        "### Rewriting PPV\n",
        "\n",
        "$$ PPV = P(pos|\\hat{pos})$$\n",
        "*pos* is actually \"positive\" and pos hat is \"predicted positive\"\n",
        "\n",
        "By Bayes rule, this is:\n",
        "\n",
        "$$PPV = \\dfrac {P(\\hat{pos}|pos * P(pos)}{P(\\hat{pos})}$$\n",
        "\n",
        "### For the numerator:\n",
        "\n",
        "$$Sensitivity = P(\\hat{pos}|pos)$$\n",
        "Keep in mind that *sensitivity* is how well the model predicts actual positive cases as positive.\n",
        "\n",
        "$$Prevalence = P(pos)$$\n",
        "*Prevalence* is how many actual positives there are in the population. \n",
        "\n",
        "### For the denominator:\n",
        "\n",
        "$$P(\\hat{pos})= TruePos+FalsePos$$\n",
        "The model's positive prediction are the sum of it when it correctly predicts positive and incorrectly predicts positive. \n",
        "\n",
        "The true positives can be written in terms of sensitivity and prevalence.\n",
        "$$TruePos = P(\\hat{pos}|pos) * P(pos)$$\n",
        "We can use substitution to get:\n",
        "$$TruePos = Sensitivity * Prevalance$$\n",
        "\n",
        "The false positives can also be written in terms of specificity and prevalence:\n",
        "\n",
        "$$FalsePos = P(\\hat{pos}|neg)*P(neg)$$\n",
        "$$1 - Specificity = P(\\hat{pos}|neg$$\n",
        "$$1 - Prevalence = P(neg)$$\n",
        "\n",
        "### PPV Rewritten:\n",
        "If substituting these into the equation we get:\n",
        "\n",
        "$$PPV = \\dfrac{Sensitivity * Prevalence}{Sensitivity * Prevalence+(1-Specificity)*(1-Prevalence)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyHpqtcksXpS",
        "colab_type": "text"
      },
      "source": [
        "## ROC Curve\n",
        "\n",
        "The ROC curve is one of the most useful tools when evaluating medical models. The ROC curve allows us to plot the sensitivity of a model against the specificity of the model at different decision thresholds. \n",
        "When we evaluate a chest X-ray image using a model, the model provides an output which is the probability of disease of such image. This output can be transformed into a ***diagnosis*** using a threshold or operating point. When the probability is above the threshold then we interpret this as positive -- the patient has a disease -- but if the probability is below the threshold we interpret this as negative -- the patient does not have a disease.  For instance, if the probability is 0.7 and the threshold is 0.5 then we would interpret the image as positive. If the probability is 0.3 and the threshold is 0.5 then we would interpret the image as negative. \n",
        "Please note that the threshold affects the metrics we use to evaluate the model. Let's say the threshold is 0 then every image would be interpret as postive so the Sensitivity would be 1 whereas the Specificity would be 0. Now let's say the threshold is 1 then every image would be classified as negative. The Sensitivity would be 0 and the specificity would be 1. "
      ]
    }
  ]
}